# Аудит логов training/self-play/eval (2026-02-19)

## 1) Что было просмотрено

- `LOGS_FOR_AGENTS.md` (1000 эпизодов текущего прогона, с `[TRAIN]` и `[SELFPLAY]`, reward-строки, mask-строки).
- `results.txt` (исторические агрегированные прогоны `winrate_mean/vp_diff_mean/reward_mean/ep_len_mean`).
- `trainRes.txt` (строки `Iteration ... ended ...`, местами смешанные/конфликтующие сообщения о победителе).
- Для проверки инвариантов дополнительно просмотрен код среды/тренировки:
  - `gym_mod/gym_mod/envs/warhamEnv.py`
  - `gym_mod/gym_mod/engine/mission.py`
  - `train.py`
  - `eval.py`

## 2) Наблюдаемые префиксы и фазы

### Перефиксы
- Есть: `[TRAIN][START]`, `[TRAIN][EP]`, `[TRAIN][SAVE]`, `[SELFPLAY]`, `[MASK][SHOOT]`.
- Нет в этом логе явных `[EVAL]` блоков (оценка видна только агрегатами в `results.txt`).

### Фазы
- В коде фазы определены и вызываются в порядке: `command -> movement -> shooting -> charge -> fight`.
- Но в текущем тренировочном логе подробные phase/turn-сообщения отключены при `trunc=True`.
- Прямое сообщение в логе: «Логи фаз/ходов отключены (trunc=True) ... VERBOSE_LOGS=1 или MANUAL_DICE=1».

## 3) Выборка последних завершённых эпизодов (20 шт: ep=981..1000)

Все 20 эпизодов завершились по лимиту хода (`turn=11`, `battle_round=11`, `end_reason=turn_limit_Only War` в `[TRAIN][EP]`).

| ep | model_hp | enemy_hp | model_vp | enemy_vp | vp_diff | win | ep_reward |
|---:|---:|---:|---:|---:|---:|---:|---:|
| 981 | 20 | 20 | 0 | 0 | 0 | 0 | -0.1000 |
| 982 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0950 |
| 983 | 20 | 20 | 1 | 0 | +1 | 1 | +0.0050 |
| 984 | 19 | 11 | 0 | 0 | 0 | 0 | +0.0420 |
| 985 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0970 |
| 986 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0350 |
| 987 | 20 | 20 | 0 | 1 | -1 | 0 | -0.0300 |
| 988 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0650 |
| 989 | 20 | 20 | 5 | 0 | +5 | 1 | +0.4550 |
| 990 | 19 | 19 | 0 | 0 | 0 | 0 | -0.0070 |
| 991 | 20 | 20 | 1 | 2 | -1 | 0 | -0.0100 |
| 992 | 20 | 20 | 1 | 0 | +1 | 1 | +0.0050 |
| 993 | 20 | 20 | 5 | 0 | +5 | 1 | +0.3650 |
| 994 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0550 |
| 995 | 19 | 8  | 3 | 0 | +3 | 1 | +0.1045 |
| 996 | 17 | 3  | 0 | 2 | -2 | 0 | +0.0015 |
| 997 | 10 | 18 | 0 | 0 | 0 | 0 | -0.1740 |
| 998 | 20 | 20 | 0 | 0 | 0 | 0 | -0.0650 |
| 999 | 20 | 20 | 1 | 0 | +1 | 1 | +0.1600 |
| 1000| 20 | 20 | 0 | 0 | 0 | 0 | -0.0600 |

Сводка по этим 20 эпизодам:
- winrate = **30%** (6/20)
- средний `ep_reward` = **+0.0173**
- средний `vp_diff` = **+0.6**
- но много «пустых» концов 20:20 по HP и 0:0 по VP.

## 4) Метрики по всему `LOGS_FOR_AGENTS.md` (1000 эпизодов)

- winrate: **11.4%** (114/1000)
- средний `ep_reward`: **-0.0628**
- средний `vp_diff`: **+0.019**
- все эпизоды завершаются по `turn_limit_Only War` (без wipeout в этом логе)

Частоты характерных событий (всего по логу):
- `hold_penalty`: 3155
- `idle вне цели penalty`: 3973
- `idle skip (hold_penalty_already_applied)`: 1502
- `стрельба: штраф за пропуск`: 34
- `[MASK][SHOOT] Нет доступных целей`: 40
- `[MASK][SHOOT] Доступные индексы`: 79
- `отступление из боя penalty`: 172
- `Reward (стрельба): damage=...`: 215
- `Reward (бой): damage=...`: 335

Интерпретация: модель часто не делает прогресса по objectives/урону и накапливает штрафы idle/hold.

## 5) Реконструкция «как реально идёт игра»

Из логов видно устойчивый паттерн:
1. Запуск с self-play против fixed_checkpoint (`opp_eps=0.0`).
2. Подробные логи фаз отключены (`trunc=True`) — остаются reward-события и конец эпизода.
3. Частые ситуации «нет целей для стрельбы» и/или «пропуск/непрогресс в движение»:
   - повторяющиеся `hold_penalty=-0.200`
   - `idle вне цели: penalty=-0.050`
4. Иногда есть всплески успешного VP (эпизоды с `vp_diff=+3/+5`) или урона в стрельбе.
5. Почти всегда бой доживает до лимита хода, после чего победитель выводится из `vp_diff` в train-логике.

## 6) Проверка инвариантов «по правилам»

### Что выглядит корректно
- Порядок фаз в `step()` строгий: command→movement→shooting→charge→fight.
- В `shooting_phase` выбор цели идёт по `valid_target_ids`, при невалидном индексе стрельба пропускается и логируется.
- В `charge_phase` цель проверяется через `chargeAble` (только допустимые цели).
- В movement есть проверка выхода за границы через `_is_model_cell_in_bounds`.
- CP-головы ограничены пространством действий (`use_cp` дискретно 0..4, `cp_on` по числу юнитов).

### Что подозрительно/неконсистентно
1. **`Конец эпизода: reason=unknown winner=None`** почти всегда, хотя `[TRAIN][EP]` пишет `end_reason=turn_limit_Only War`.
   - Похоже на рассинхрон источников end_reason/winner в train-логировании.
2. **Недостаточная наблюдаемость правил в train-режиме**: из-за `trunc=True` нет пошаговых phase/action логов, сложно доказуемо валидировать LOS/range/charge по каждому действию.
3. **В `trainRes.txt` есть конфликтующие сообщения `model won!/enemy won!/draw!` подряд** для одной итерации — вероятно смесь сообщений из нескольких env/итераций.

## 7) Главные проблемы, бьющие по winrate

1. **Сверхчастые штрафы за «бездействие вне objectives»** (idle/hold), агент застревает в неэффективных траекториях.
2. **Слишком много эпизодов с нулевым контактом** (20:20 HP, 0:0 VP к лимиту хода).
3. **Слабая диагностируемость фазовых ошибок** в `trunc=True`: трудно локально чинить policy, нет причинно-следственного трейсинга по action heads.
4. **Непрозрачность победителя в runtime-логе** (`reason=unknown`) — усложняет корректную оценку и отладку.
5. **Сигнал reward может быть перекошен в «штрафной шум»** (много penalty событий), что ухудшает обучаемость.

## 8) Рекомендации по росту winrate (без ломания правил)

### A. Безопасные (высокий эффект, низкий риск)
1. **В eval всегда greedy**: `FORCE_GREEDY=1`, `EVAL_EPSILON=0.0`.
2. **Сохранить/усилить action masking для стрельбы и добавить аналогичную маску для charge target head** (если её нет на уровне policy выбора), чтобы убрать невалидные таргеты.
3. **Починить/унифицировать end_reason/winner в train-логах** (источник истины из env info).
4. **Добавить диагностический режим 1–5% эпизодов с подробными phase-логами при training** (без полного VERBOSE для всех env).
5. **Явно логировать долю невалидных/пропущенных действий по головам action-space на эпизод**.

### B. Умеренные (средний риск)
6. **Тонко ослабить частоту/вес idle/hold penalty**, чтобы агент не «тонул» в штрафах раньше, чем научится занимать objective.
7. **Добавить reward shaping за прогресс к objective на уровне шага** (не меняя победные условия миссии).
8. **Штраф за явный skip в shooting/charge оставить, но делать его контекстным** (если действительно были валидные цели).

### C. Алгоритмические (после стабилизации правил и логов)
9. Для текущего DDQN+Dueling+PER+n-step: попробовать
   - `batch_size`: 256→384
   - `target_update`: чаще (например каждые 500–1000 updates)
   - `epsilon` decay чуть быстрее до 0.05
   - PER: снизить `alpha` к 0.5–0.6 при сильном шуме reward.
10. Проверить update ratio (updates per env step) и избегать недообучения при 16 env.

## 9) Какие строки логировать дополнительно

Минимальный набор на эпизод:
- `action_heads={move,attack,shoot,charge,use_cp,cp_on}` (сырые индексы)
- `valid_masks_sizes` + `num_valid` по head (особенно shoot/charge)
- `invalid_reason` (например `shoot_target_out_of_valid_range`, `charge_target_not_chargeable`)
- `phase_summary`: сколько было `movement/shooting/charge/fight` действий и сколько из них skip/invalid
- `termination`: `env_end_reason`, `winner`, `vp_diff`, `hp_totals`

Это даст возможность быстро понять: policy ошибается в выборе, маски слабые, или reward не ведёт к цели.
