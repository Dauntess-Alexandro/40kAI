diff --git a/gym_mod/gym_mod/envs/warhamEnv.py b/gym_mod/gym_mod/envs/warhamEnv.py
index b4b3f7997bbf9dd22ee623f88f5a014f7839e7df..b85d19b2e7fc069376058d8bdc04a065e6adbca8 100644
--- a/gym_mod/gym_mod/envs/warhamEnv.py
+++ b/gym_mod/gym_mod/envs/warhamEnv.py
@@ -1,45 +1,46 @@
 import gymnasium as gym
 from gymnasium import spaces
 import numpy as np
 import matplotlib
 matplotlib.use("Agg")
 import matplotlib.pyplot as plt
 import os
 import random
 import re
 from typing import Optional
 
 import reward_config as reward_cfg
 
 from ..engine.utils import *
 from ..engine import utils as engine_utils
 from gym_mod.engine.mission import (
     MISSION_NAME,
     MAX_BATTLE_ROUNDS,
     score_end_of_command_phase,
     apply_end_of_battle,
+    controlled_objectives,
 )
 from gym_mod.engine.skills import apply_end_of_command_phase
 from gym_mod.engine.logging_utils import format_unit
 from gym_mod.engine.state_export import write_state_json
 from gym_mod.engine.game_io import get_active_io
 
 # ============================================================
 # üîß FIX: resolve string weapons like "Bolt pistol [PISTOL]"
 # so engine.utils.attack() always receives a dict (or we safely
 # skip the attack instead of crashing with "'str' object has no attribute 'get'").
 # This is intentionally defensive: if WeaponData can't be found,
 # we still won't crash during training.
 # ============================================================
 
 _attack_original = attack  # keep reference to the original engine attack
 _WEAPON_INDEX = None
 
 def _norm_weapon_name(x):
     if not isinstance(x, str):
         return x
     # remove tags in square brackets: "Bolt pistol [PISTOL]" -> "Bolt pistol"
     x = re.sub(r"\s*\[.*?\]\s*", "", x)
     return x.strip().lower()
 
 def _build_weapon_index():
@@ -832,50 +833,79 @@ class Warhammer40kEnv(gym.Env):
             "battle round": self.battle_round,
             "active side": self.active_side,
             "phase": self.phase,
         }
 
     def _should_log(self) -> bool:
         if self._is_verbose():
             return True
         return self.trunc is False
 
     def _is_verbose(self) -> bool:
         return os.getenv("VERBOSE_LOGS", "0") == "1" or os.getenv("MANUAL_DICE", "0") == "1"
 
     def _ensure_io(self):
         if not hasattr(self, "io") or self.io is None:
             self.io = get_active_io()
         return self.io
 
     def _log(self, msg: str, verbose_only: bool = False):
         if verbose_only and not self._is_verbose():
             return
         if not self._should_log():
             return
         self._ensure_io().log(msg)
 
+    def _unit_max_hp(self, side: str, idx: int) -> float:
+        data_list = self.unit_data if side == "model" else self.enemy_data
+        if not (0 <= idx < len(data_list)):
+            return 1.0
+        unit_data = data_list[idx]
+        if not isinstance(unit_data, dict):
+            return 1.0
+        wounds = float(unit_data.get("W", 1))
+        models = float(unit_data.get("#OfModels", 1))
+        return max(1.0, wounds * models)
+
+    def _melee_strength_score(self, side: str, idx: int) -> float:
+        weapons = self.unit_melee if side == "model" else self.enemy_melee
+        data_list = self.unit_data if side == "model" else self.enemy_data
+        if not (0 <= idx < len(weapons)) or not (0 <= idx < len(data_list)):
+            return 0.0
+        weapon = weapons[idx]
+        if not isinstance(weapon, dict):
+            return 0.0
+        attacks = float(weapon.get("A", 1))
+        strength = float(weapon.get("S", 1))
+        damage = float(weapon.get("Damage", 1))
+        ws = float(weapon.get("WS", 4))
+        ap = float(weapon.get("AP", 0))
+        hit_chance = max(0.0, min(1.0, (7.0 - ws) / 6.0))
+        ap_bonus = 1.0 + max(0.0, -ap) * 0.05
+        models = float(data_list[idx].get("#OfModels", 1)) if isinstance(data_list[idx], dict) else 1.0
+        return attacks * strength * damage * hit_chance * ap_bonus * models
+
     def _log_phase(self, side: str, phase: str):
         if not self._should_log():
             return
         phase_title = {
             "command": "–§–ê–ó–ê –ö–û–ú–ê–ù–î–û–í–ê–ù–ò–Ø",
             "movement": "–§–ê–ó–ê –î–í–ò–ñ–ï–ù–ò–Ø",
             "shooting": "–§–ê–ó–ê –°–¢–†–ï–õ–¨–ë–´",
             "charge": "–§–ê–ó–ê –ß–ê–†–î–ñ–ê",
             "fight": "–§–ê–ó–ê –ë–û–Ø",
         }.get(phase, f"–§–ê–ó–ê {phase.upper()}")
         self._log(f"--- {phase_title} ---")
 
     def _log_unit(self, side: str, unit_id: int, unit_idx: int, msg: str):
         if not self._should_log():
             return
         side_label = self._side_label(side)
         unit_label = self._format_unit_label(side, unit_idx, unit_id=unit_id)
         self._log(f"[{side_label}] {unit_label}: {msg}")
 
     def _display_side(self, side: str) -> str:
         if side == "enemy":
             return "PLAYER"
         if side == "model":
             return "MODEL"
         return side.upper()
@@ -2316,71 +2346,129 @@ class Warhammer40kEnv(gym.Env):
                             self.enemyCharged[i] = 1
                             pos_after = tuple(self.enemy_coords[i])
                             self._log_unit_phase(
                                 self._display_side("enemy"),
                                 "charge",
                                 i + 21,
                                 i,
                                 f"Charge move: from {pos_before} -> {pos_after}, ended_in_engagement={self.enemyInAttack[i][0] == 1}.",
                             )
                             # 10e: Heroic Intervention –¥–æ—Å—Ç—É–ø–µ–Ω –∑–∞—â–∏—Ç–Ω–∏–∫—É –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ charge move.
                             self._resolve_heroic_intervention(
                                 defender_side="model",
                                 charging_side="enemy",
                                 charging_idx=i,
                                 phase="charge",
                                 manual=False,
                             )
                         elif self.trunc is False:
                             self._log(
                                 f"{self._format_unit_label('enemy', i)} –Ω–µ —Å–º–æ–≥ –∑–∞—á–∞—Ä–¥–∂–∏—Ç—å {self._format_unit_label('model', idOfM)} (–±—Ä–æ—Å–æ–∫ {diceRoll} vs –Ω—É–∂–Ω–æ {required:.1f})"
                             )
         return None
 
     def fight_phase(self, side: str):
         self.begin_phase(side, "fight")
+        reward_delta = 0.0
+        engaged_pairs = []
+        pre_enemy_hp_total = None
+        pre_model_hp_total = None
+        pre_enemy_dead = None
+        pre_obj_controlled = None
+        advantage_term = 0.0
+        strength_term = 0.0
         if side == "model":
             self._log_phase("MODEL", "fight")
             engaged_model = [i for i in range(len(self.unit_health)) if self.unit_health[i] > 0 and self.unitInAttack[i][0] == 1]
             engaged_enemy = [i for i in range(len(self.enemy_health)) if self.enemy_health[i] > 0 and self.enemyInAttack[i][0] == 1]
             if not engaged_model and not engaged_enemy:
                 self._log("[MODEL] –ë–ª–∏–∂–Ω–∏–π –±–æ–π: –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Ç–∞–∫")
             else:
                 model_list = self._format_unit_choices("model", engaged_model)
                 enemy_list = self._format_unit_choices("enemy", engaged_enemy)
                 self._log(f"[MODEL] –ë–ª–∏–∂–Ω–∏–π –±–æ–π: —É—á–∞—Å—Ç–≤—É—é—Ç {model_list}; –ø—Ä–æ—Ç–∏–≤–Ω–∏–∫–∏ {enemy_list}")
                 for idx in engaged_model:
                     def_idx = self.unitInAttack[idx][1]
                     if 0 <= def_idx < len(self.enemy_health):
                         self._log_unit(
                             "MODEL",
                             idx + 21,
                             idx,
                             f"–í –±–æ—é —Å {self._format_unit_label('enemy', def_idx)}",
                         )
+                        engaged_pairs.append((idx, def_idx))
+                if engaged_pairs:
+                    pre_enemy_hp_total = float(sum(self.enemy_health))
+                    pre_model_hp_total = float(sum(self.unit_health))
+                    pre_enemy_dead = sum(1 for hp in self.enemy_health if hp <= 0)
+                    self.refresh_objective_control()
+                    pre_obj_controlled, _ = controlled_objectives(self, "model")
+                    for model_idx, enemy_idx in engaged_pairs:
+                        model_max_hp = self._unit_max_hp("model", model_idx)
+                        enemy_max_hp = self._unit_max_hp("enemy", enemy_idx)
+                        model_hp_frac = float(self.unit_health[model_idx]) / model_max_hp
+                        enemy_hp_frac = float(self.enemy_health[enemy_idx]) / enemy_max_hp
+                        advantage = max(-1.0, min(1.0, model_hp_frac - enemy_hp_frac))
+                        advantage_term += reward_cfg.MELEE_ADVANTAGE_SCALE * advantage
+                        model_power = self._melee_strength_score("model", model_idx)
+                        enemy_power = self._melee_strength_score("enemy", enemy_idx)
+                        if model_power > enemy_power:
+                            strength_term += reward_cfg.MELEE_STRENGTH_SCALE
+                        elif model_power < enemy_power:
+                            strength_term -= reward_cfg.MELEE_STRENGTH_SCALE
         self.resolve_fight_phase(active_side=side, trunc=self.trunc)
 
+        if side == "model" and engaged_pairs:
+            post_enemy_hp_total = float(sum(self.enemy_health))
+            post_model_hp_total = float(sum(self.unit_health))
+            post_enemy_dead = sum(1 for hp in self.enemy_health if hp <= 0)
+
+            damage_dealt = max(0.0, pre_enemy_hp_total - post_enemy_hp_total)
+            damage_taken = max(0.0, pre_model_hp_total - post_model_hp_total)
+            damage_dealt_norm = damage_dealt / max(1.0, float(self.enemy_hp_max_total))
+            damage_taken_norm = damage_taken / max(1.0, float(self.model_hp_max_total))
+            damage_term = reward_cfg.MELEE_REWARD_DAMAGE_SCALE * damage_dealt_norm
+            taken_term = reward_cfg.MELEE_REWARD_TAKEN_SCALE * damage_taken_norm
+            kill_delta = max(0, post_enemy_dead - pre_enemy_dead)
+            kill_term = reward_cfg.MELEE_REWARD_KILL_BONUS * kill_delta
+
+            self.refresh_objective_control()
+            post_obj_controlled, _ = controlled_objectives(self, "model")
+            obj_delta = post_obj_controlled - (pre_obj_controlled or 0)
+            obj_term = reward_cfg.MELEE_OBJECTIVE_CONTROL_SCALE * obj_delta
+
+            reward_delta += damage_term + kill_term - taken_term + advantage_term + strength_term + obj_term
+            self._log(
+                "Reward (–±–æ–π): "
+                f"damage={damage_term:.3f} (norm={damage_dealt_norm:.3f}, dealt={damage_dealt:.2f}), "
+                f"kills={kill_term:.3f} (delta={kill_delta}), "
+                f"taken=-{taken_term:.3f} (norm={damage_taken_norm:.3f}, taken={damage_taken:.2f}), "
+                f"advantage={advantage_term:.3f}, strength={strength_term:.3f}, "
+                f"objectives={obj_term:.3f} (delta={obj_delta}), total={reward_delta:.3f}"
+            )
+        return reward_delta
+
     def refresh_objective_control(self):
         self.model_obj_oc = np.zeros(len(self.coordsOfOM), dtype=int)
         self.enemy_obj_oc = np.zeros(len(self.coordsOfOM), dtype=int)
 
         for i in range(len(self.unit_health)):
             if self.unit_health[i] <= 0:
                 continue
             wounds = self.unit_data[i]["W"]
             remaining_models = (self.unit_health[i] + wounds - 1) // wounds
             effective_oc = self.modelOC[i] * remaining_models
             if effective_oc <= 0:
                 continue
             for j in range(len(self.coordsOfOM)):
                 if distance(self.coordsOfOM[j], self.unit_coords[i]) <= 5:
                     self.model_obj_oc[j] += effective_oc
 
         for i in range(len(self.enemy_health)):
             if self.enemy_health[i] <= 0:
                 continue
             wounds = self.enemy_data[i]["W"]
             remaining_models = (self.enemy_health[i] + wounds - 1) // wounds
             effective_oc = self.enemyOC[i] * remaining_models
             if effective_oc <= 0:
                 continue
             for j in range(len(self.coordsOfOM)):
@@ -2774,51 +2862,51 @@ class Warhammer40kEnv(gym.Env):
                         fought_enemy.add(i)
                 next_side = "model"
 
         # –ø–æ—Å–ª–µ Fight Phase ‚Äî charged —Å–±—Ä–∞—Å—ã–≤–∞–µ–º (–Ω–∞ –≤—Å—è–∫–∏–π)
         self.unitCharged = [0] * len(self.unit_health)
         self.enemyCharged = [0] * len(self.enemy_health)
 
         if quiet is False:
             self._log("‚öîÔ∏è Combat resolution complete.\n")
 
 
 
     def step(self, action):
         reward = 0
         res = 0
         model_hp_start = float(sum(self.unit_health))
         self.unitCharged = [0] * len(self.unit_health)
         self.enemyCharged = [0] * len(self.enemy_health)
         self.active_side = "model"
         battle_shock, delta = self.command_phase("model", action=action)
         reward += delta
         advanced_flags, delta = self.movement_phase("model", action=action, battle_shock=battle_shock)
         reward += delta
         reward += self.shooting_phase("model", advanced_flags=advanced_flags, action=action) or 0
         reward += self.charge_phase("model", advanced_flags=advanced_flags, action=action) or 0
-        self.fight_phase("model")
+        reward += self.fight_phase("model") or 0
         game_over, _, winner = apply_end_of_battle(self, log_fn=self._log)
         self.enemyStrat["overwatch"] = -1
         self.enemyStrat["smokescreen"] = -1
 
         for i in range(len(self.unit_health)):
             if self.unit_health[i] < 0:
                 self.unit_health[i] = 0
         for i in range(len(self.enemy_health)):
             if self.enemy_health[i] < 0:
                 self.enemy_health[i] = 0
 
         model_hp_end = float(sum(self.unit_health))
         damage_taken = max(0.0, model_hp_start - model_hp_end)
         if damage_taken > 0:
             damage_taken_norm = damage_taken / max(1.0, float(self.model_hp_max_total))
             penalty = reward_cfg.DAMAGE_TAKEN_SCALE * damage_taken_norm
             reward -= penalty
             self._log(
                 "Reward (—É—Ä–æ–Ω –ø–æ –º–æ–¥–µ–ª–∏): "
                 f"damage_taken={damage_taken:.2f}, norm={damage_taken_norm:.3f}, penalty=-{penalty:.3f}"
             )
 
         if game_over:
             res = 4
             if winner == "model":
diff --git a/reward_config.py b/reward_config.py
index 04892b5719376d657fab8ec57a526c6c3d296913..d688c9834bd45af055f69c70160602cfe60fda15 100644
--- a/reward_config.py
+++ b/reward_config.py
@@ -1,20 +1,31 @@
 """
 –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è reward-—à—ç–π–ø–∏–Ω–≥–∞.
 –ú–µ–Ω—è–π—Ç–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –Ω–µ –ª–µ–∑—Ç—å –≤ –∫–æ–¥ —Å—Ä–µ–¥—ã.
 """
 
 # =========================
 # Reward shaping (shooting)
 # =========================
 # –ë–∞–∑–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã ‚Äî –ø–æ–¥–±–∏—Ä–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ.
 SHOOT_REWARD_DAMAGE_SCALE = 0.6
 SHOOT_REWARD_KILL_BONUS = 0.4
 SHOOT_REWARD_OVERKILL_PENALTY = 0.2
 SHOOT_REWARD_SKIP_PENALTY = 0.15
 SHOOT_REWARD_TARGET_LOW_HP = 0.05
 SHOOT_REWARD_TARGET_ON_OBJ = 0.07
 SHOOT_REWARD_TARGET_HIGH_OC = 0.05
 SHOOT_REWARD_ACTION_BONUS = 0.0
 
 # Penalize damage received during the model's step (normalized by model total max HP).
 DAMAGE_TAKEN_SCALE = 0.5
+
+# ======================
+# Reward shaping (fight)
+# ======================
+# –ë–∞–∑–æ–≤—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã ‚Äî –ø–æ–¥–±–∏—Ä–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ.
+MELEE_REWARD_DAMAGE_SCALE = 0.6
+MELEE_REWARD_KILL_BONUS = 0.4
+MELEE_REWARD_TAKEN_SCALE = 0.5
+MELEE_ADVANTAGE_SCALE = 0.15
+MELEE_STRENGTH_SCALE = 0.1
+MELEE_OBJECTIVE_CONTROL_SCALE = 0.2

