diff --git a/eval.py b/eval.py
index 85fb0706596ee635257e815505e45f2f908775e3..7b8efb6729eb0cee2f4e3d02167dd3571ea3314c 100644
--- a/eval.py
+++ b/eval.py
@@ -1,30 +1,31 @@
 import argparse
 import os
 import pickle
 import sys
 from collections import Counter
+from statistics import median
 from typing import Optional
 
 import torch
 
 from gym_mod.engine.deployment import deploy_only_war, post_deploy_setup
 from gym_mod.engine.mission import check_end_of_battle
 from gym_mod.envs.warhamEnv import roll_off_attacker_defender
 from model.DQN import DQN
 from model.utils import build_shoot_action_mask, convertToDict, unwrap_env
 
 
 def log(message: str) -> None:
     if message.startswith("["):
         print(f"[EVAL]{message}", flush=True)
     else:
         print(f"[EVAL] {message}", flush=True)
 
 
 def load_latest_model(model_path: Optional[str] = None):
     if model_path and model_path != "None":
         pickle_path = model_path
         checkpoint_path = model_path[:-len("pickle")] + "pth"
     else:
         save_path = "models/"
         folders = os.listdir(save_path) if os.path.isdir(save_path) else []
@@ -123,52 +124,54 @@ def run_episode(env, model_units, enemy_units, policy_net, epsilon, device):
             info = env_unwrapped.get_info()
             break
 
         state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)
         shoot_mask = build_shoot_action_mask(env)
         action = select_action_with_epsilon(
             env,
             state_tensor,
             policy_net,
             epsilon,
             len(model_units),
             shoot_mask=shoot_mask,
         )
         action_dict = convertToDict(action)
         next_observation, _, done, _, info = env.step(action_dict)
         state = next_observation
 
     end_reason = info.get("end reason", "")
     winner = info.get("winner")
     if not end_reason or winner is None:
         _, fallback_reason, fallback_winner = check_end_of_battle(env_unwrapped)
         if not end_reason:
             end_reason = fallback_reason
         if winner is None:
             winner = fallback_winner
-    vp_diff = info.get("model VP", 0) - info.get("player VP", 0)
-    return winner, end_reason or "unknown", vp_diff
+    model_vp = info.get("model VP", 0)
+    enemy_vp = info.get("player VP", 0)
+    vp_diff = model_vp - enemy_vp
+    return winner, end_reason or "unknown", vp_diff, model_vp, enemy_vp
 
 
 def main():
     parser = argparse.ArgumentParser()
     parser.add_argument("--games", type=int, default=50)
     parser.add_argument("--model", type=str, default=None)
     args = parser.parse_args()
 
     games = args.games
     if games < 1:
         log("Некорректное значение N. Укажите число >= 1.")
         return 0
 
     if os.getenv("FORCE_GREEDY", "0") == "1":
         epsilon = 0.0
     else:
         epsilon_raw = os.getenv("EVAL_EPSILON", "0")
         epsilon = float(epsilon_raw) if epsilon_raw else 0.0
 
     os.environ.setdefault("MANUAL_DICE", "0")
 
     env, model_units, enemy_units, checkpoint = load_latest_model(args.model)
     if env is None:
         log("Модель не найдена. Проверьте папку models/ и наличие файлов .pickle/.pth.")
         return 0
@@ -197,50 +200,70 @@ def main():
     n_observations = len(state)
 
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 
     net_type = checkpoint.get("net_type") if isinstance(checkpoint, dict) else None
     dueling = net_type == "dueling"
     if not dueling and isinstance(checkpoint, dict):
         policy_state = checkpoint.get("policy_net", {})
         if any(key.startswith("value_heads.") for key in policy_state):
             dueling = True
 
     policy_net = DQN(n_observations, n_actions, dueling=dueling).to(device)
     target_net = DQN(n_observations, n_actions, dueling=dueling).to(device)
     optimizer = torch.optim.Adam(policy_net.parameters())
 
     policy_net.load_state_dict(checkpoint["policy_net"])
     target_net.load_state_dict(checkpoint["target_net"])
     optimizer.load_state_dict(checkpoint["optimizer"])
 
     policy_net.eval()
     target_net.eval()
 
     log(f"Старт оценки: игр={games}, epsilon={epsilon:.3f}.")
 
     wins = 0
+    losses = 0
+    draws = 0
     vp_diffs = []
     end_reasons = Counter()
 
     for idx in range(1, games + 1):
-        winner, end_reason, vp_diff = run_episode(
+        winner, end_reason, vp_diff, model_vp, enemy_vp = run_episode(
             env, model_units, enemy_units, policy_net, epsilon, device
         )
         vp_diffs.append(vp_diff)
         end_reasons[end_reason] += 1
         if winner == "model":
             wins += 1
-        log(f"Игра {idx}/{games}: winner={winner} vp_diff={vp_diff} end_reason={end_reason}")
+        elif winner == "enemy":
+            losses += 1
+        else:
+            draws += 1
+        log(
+            "Игра "
+            f"{idx}/{games}: "
+            f"winner={winner} "
+            f"model_vp={model_vp} "
+            f"enemy_vp={enemy_vp} "
+            f"vp_diff_model_minus_enemy={vp_diff} "
+            f"end_reason={end_reason}"
+        )
 
-    winrate = wins / games if games else 0.0
+    winrate_all = wins / games if games else 0.0
+    winrate_no_draw = wins / (wins + losses) if (wins + losses) else 0.0
     avg_vp_diff = sum(vp_diffs) / len(vp_diffs) if vp_diffs else 0.0
+    median_vp_diff = median(vp_diffs) if vp_diffs else 0.0
     log(
         "[SUMMARY] "
-        f"winrate={winrate:.3f} avg_vp_diff={avg_vp_diff:.3f} "
+        f"wins={wins} losses={losses} draws={draws} "
+        f"winrate_all={winrate_all:.3f} "
+        f"winrate_no_draw={winrate_no_draw:.3f} "
+        f"avg_vp_diff={avg_vp_diff:.3f} "
+        f"median_vp_diff={median_vp_diff:.3f} "
         f"end_reasons={dict(end_reasons)}"
     )
     return 0
 
 
 if __name__ == "__main__":
     sys.exit(main())

