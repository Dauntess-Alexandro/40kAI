diff --git a/model/utils.py b/model/utils.py
index 6eb0b39feeb3aa431a06ac43659d80145ff3a3f3..5b8407f6ad784f8100e2ff415764a2a851be9c57 100644
--- a/model/utils.py
+++ b/model/utils.py
@@ -63,88 +63,103 @@ def select_action(env, state, steps_done, policy_net, len_model, shoot_mask=None
                 action.append(int(head.argmax().item()))
             return torch.tensor([action], device="cpu")
     else:
         sampled_action = env.action_space.sample()
         shoot_choice = sampled_action["shoot"]
         if shoot_mask is not None:
             mask = torch.as_tensor(shoot_mask, dtype=torch.bool)
             valid_indices = torch.where(mask)[0].tolist()
             if valid_indices:
                 shoot_choice = random.choice(valid_indices)
         action_list = [
             sampled_action["move"],
             sampled_action["attack"],
             shoot_choice,
             sampled_action["charge"],
             sampled_action["use_cp"],
             sampled_action["cp_on"],
         ]
         for i in range(len_model):
             label = "move_num_" + str(i)
             action_list.append(sampled_action[label])
         action = torch.tensor([action_list], device="cpu")
         return action
 
 def build_shoot_action_mask(env, log_fn=None, debug=False):
+    def maybe_log_mask_state(state_key, message):
+        if log_fn is None:
+            return
+        last_state = getattr(env, "_last_shoot_mask_log_state", None)
+        if last_state != state_key:
+            env._last_shoot_mask_log_state = state_key
+            log_fn(message)
+
     shoot_space = env.action_space.spaces["shoot"].n
     valid_lengths = []
     for i in range(len(env.unit_health)):
         if env.unit_health[i] <= 0:
             continue
         if env.unitFellBack[i]:
             continue
         if env.unitInAttack[i][0] == 1:
             continue
         if env.unit_weapon[i] == "None":
             continue
         valid_targets = []
         for j in range(len(env.enemy_health)):
             if (
                 distance(env.unit_coords[i], env.enemy_coords[j]) <= env.unit_weapon[i]["Range"]
                 and env.enemy_health[j] > 0
                 and env.enemyInAttack[j][0] == 0
             ):
                 valid_targets.append(j)
         if valid_targets:
             valid_lengths.append(len(valid_targets))
     if not valid_lengths:
-        if debug and log_fn is not None:
-            log_fn("[MASK][SHOOT] Нет доступных целей для стрельбы (маска не применяется).")
+        maybe_log_mask_state(
+            ("none", "no_targets"),
+            "[MASK][SHOOT] Нет доступных целей для стрельбы (маска не применяется).",
+        )
         return None
     min_len = min(valid_lengths)
     if min_len <= 0:
-        if debug and log_fn is not None:
-            log_fn("[MASK][SHOOT] Нулевая длина маски (маска не применяется).")
+        maybe_log_mask_state(
+            ("none", "zero_len"),
+            "[MASK][SHOOT] Нулевая длина маски (маска не применяется).",
+        )
         return None
     mask = torch.zeros(shoot_space, dtype=torch.bool)
     mask[:min_len] = True
+    mask_state = ("mask", min_len, len(valid_lengths), shoot_space)
+    maybe_log_mask_state(
+        mask_state,
+        "[MASK][SHOOT] "
+        f"Доступные индексы: 0..{min_len - 1}, "
+        f"юнитов с целями={len(valid_lengths)}, размер пространства={shoot_space}.",
+    )
     if debug and log_fn is not None:
-        log_fn(
-            "[MASK][SHOOT] "
-            f"Доступные индексы: 0..{min_len - 1}, "
-            f"юнитов с целями={len(valid_lengths)}, размер пространства={shoot_space}."
-        )
+        log_fn(f"[MASK][SHOOT][DEBUG] Полная маска: {mask.tolist()}")
     return mask
 
 def convertToDict(action):
     naction = action.numpy()[0]
     action_dict = {
         'move': naction[0],
         'attack': naction[1],
         'shoot': naction[2],
         'charge': naction[3],
         'use_cp': naction[4],
         'cp_on': naction[5]
     }
     for i in range(len(naction)-6):
         label = "move_num_"+str(i)
         action_dict[label] = naction[i+6]
     return action_dict
 
 def optimize_model(
     policy_net,
     target_net,
     optimizer,
     memory,
     n_obs,
     double_dqn_enabled=True,
     per_enabled=False,

