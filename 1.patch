diff --git a/model/utils.py b/model/utils.py
index 487febcb758c11f3de25b7a62e0972e8af597330..d3bc9f073b54ccbc6f0b6e6f426ba2f2514fc2c0 100644
--- a/model/utils.py
+++ b/model/utils.py
@@ -174,44 +174,45 @@ def optimize_model(
                 ]
                 max_per_head = torch.stack(next_q, dim=1)  # [N, num_heads]
             else:
                 max_per_head = torch.stack(
                     [h.max(1).values for h in target_next], dim=1
                 )  # [N, num_heads]
             next_state_values[non_final_mask] = max_per_head
 
     gamma_n = GAMMA ** n_step_batch
     expected_state_action_values = reward_batch.unsqueeze(1) + (gamma_n.unsqueeze(1) * next_state_values)  # [B, num_heads]
 
     if per_enabled:
         loss_per_element = F.smooth_l1_loss(
             selected_action_values, expected_state_action_values, reduction="none"
         )
         per_sample_loss = loss_per_element.mean(dim=1)  # [B]
         weight_t = torch.tensor(weights, device=dev, dtype=torch.float32)
         loss = (per_sample_loss * weight_t).mean()
         td_errors = (selected_action_values - expected_state_action_values).abs().mean(dim=1)
         new_priorities = td_errors.detach().cpu().numpy() + per_eps
         memory.update_priorities(indices, new_priorities)
         per_stats = {
             "priority_mean": float(new_priorities.mean()),
             "priority_max": float(new_priorities.max()),
             "is_weight_mean": float(weight_t.mean().item()),
+            "is_weight_max": float(weight_t.max().item()),
             "td_error_mean": float(td_errors.mean().item()),
             "td_error_max": float(td_errors.max().item()),
         }
     else:
         criterion = nn.SmoothL1Loss()
         loss = criterion(selected_action_values, expected_state_action_values)
         per_stats = None
 
     optimizer.zero_grad()
     loss.backward()
     torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)
     optimizer.step()
 
     return {
         "loss": loss.item(),
         "td_target_mean": expected_state_action_values.mean().item(),
         "td_target_max": expected_state_action_values.max().item(),
         "per_stats": per_stats,
     }
diff --git a/train.py b/train.py
index 4143b454dd2431b62b72cd90210edd4976170bd8..6f0787d093a8d8aaffbdb40e16cae8c7db0baf19 100644
--- a/train.py
+++ b/train.py
@@ -1,69 +1,81 @@
 from collections import Counter
 import sys
 import os
 import csv
 import numpy as np
 import gymnasium as gym
 import pickle
 import datetime
 import collections
+import math
 import json
 import random
 import matplotlib.pyplot as plt
 from tqdm import tqdm
 from gym_mod.envs.warhamEnv import *
 from gym_mod.engine import genDisplay, Unit, unitData, weaponData, initFile, metrics
 from gym_mod.engine.deployment import deploy_only_war, post_deploy_setup
 from gymnasium import spaces
 
 from model.DQN import *
 from model.memory import *
 from model.utils import *
 
 import torch
 import torch.nn as nn
 import torch.optim as optim
 import torch.nn.functional as F
 device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
 import torch
 print("[DEVICE CHECK] cuda:", torch.cuda.is_available())
 if torch.cuda.is_available():
     print("[DEVICE CHECK] name:", torch.cuda.get_device_name(0))
 
 
 import warnings
 warnings.filterwarnings("ignore") 
 
 with open(os.path.abspath("hyperparams.json")) as j:
     data = json.loads(j.read())
 
 # ===== algo flags =====
 DOUBLE_DQN_ENABLED = os.getenv("DOUBLE_DQN_ENABLED", "1") == "1"
 DUELING_ENABLED = os.getenv("DUELING_ENABLED", "0") == "1"
 REWARD_DEBUG = os.getenv("REWARD_DEBUG", "0") == "1"
 REWARD_DEBUG_EVERY = int(os.getenv("REWARD_DEBUG_EVERY", "200"))
+# ===== train logging =====
+TRAIN_LOG_ENABLED = os.getenv("TRAIN_LOG_ENABLED", "1") == "1"
+TRAIN_LOG_EVERY_UPDATES = int(os.getenv("TRAIN_LOG_EVERY_UPDATES", "200"))
+TRAIN_LOG_TO_FILE = os.getenv("TRAIN_LOG_TO_FILE", "1") == "1"
+TRAIN_LOG_TO_CONSOLE = os.getenv("TRAIN_LOG_TO_CONSOLE", "0") == "1"
+TRAIN_DEBUG = os.getenv("TRAIN_DEBUG", "0") == "1"
+if TRAIN_LOG_EVERY_UPDATES < 1:
+    TRAIN_LOG_EVERY_UPDATES = 1
+if TRAIN_DEBUG:
+    TRAIN_LOG_EVERY_UPDATES = min(TRAIN_LOG_EVERY_UPDATES, 50)
+# =========================
 # ===== per + n-step =====
 PER_ENABLED = os.getenv("PER_ENABLED", "0") == "1"
 PER_ALPHA = float(os.getenv("PER_ALPHA", "0.6"))
 PER_BETA_START = float(os.getenv("PER_BETA_START", "0.4"))
 PER_BETA_FRAMES = int(os.getenv("PER_BETA_FRAMES", "200000"))
 PER_EPS = float(os.getenv("PER_EPS", "1e-6"))
 N_STEP = int(os.getenv("N_STEP", "1"))
 if N_STEP < 1:
     N_STEP = 1
 # ======================
 
 # ===== perf knobs =====
 RENDER_EVERY = int(os.getenv("RENDER_EVERY", "20"))  # 0 = выключить рендер полностью
 UPDATES_PER_STEP = int(os.getenv("UPDATES_PER_STEP", "4"))  
 # ======================
 
 # ===== self-play config =====
 SELF_PLAY_ENABLED = os.getenv("SELF_PLAY_ENABLED", "0") == "1"
 SELF_PLAY_UPDATE_EVERY_EPISODES = int(os.getenv("SELF_PLAY_UPDATE_EVERY_EPISODES", "50"))
 SELF_PLAY_OPPONENT_MODE = os.getenv("SELF_PLAY_OPPONENT_MODE", "snapshot")
 SELF_PLAY_FIXED_PATH = os.getenv("SELF_PLAY_FIXED_PATH", "")
 SELF_PLAY_OPPONENT_EPSILON = float(os.getenv("SELF_PLAY_OPPONENT_EPSILON", "0.0"))
 
 if SELF_PLAY_UPDATE_EVERY_EPISODES < 1:
     SELF_PLAY_UPDATE_EVERY_EPISODES = 1
@@ -188,80 +200,88 @@ def save_extra_metrics(run_id: str, ep_rows: list[dict], metrics_dir="metrics"):
     plt.savefig(os.path.join(metrics_dir, f"vpdiff_{run_id}.png"))
     plt.savefig(os.path.join("gui/img", f"vpdiff_{run_id}.png"))
     plt.savefig(os.path.join("gui/img", "vpdiff.png"))
     plt.close()
 
     # --- End reasons bar ---
     reasons = [r["end_reason"] for r in ep_rows]
     c = Counter(reasons)
     keys = sorted(c.keys())
     vals = [c[k] for k in keys]
 
     plt.figure()
     plt.bar(keys, vals)
     plt.xticks(rotation=30, ha="right")
     plt.ylabel("Count")
     plt.title("End reasons")
     plt.tight_layout()
 
     plt.savefig(os.path.join(metrics_dir, f"endreasons_{run_id}.png"))
     plt.savefig(os.path.join("gui/img", f"endreasons_{run_id}.png"))
     plt.savefig(os.path.join("gui/img", "endreasons.png"))
     plt.close()
 
     print(f"[metrics] saved: {csv_path}")
 
-def append_log_for_agents(message: str, log_path: str = "LOGS_FOR_AGENTS.md"):
+def append_agent_log(line: str) -> None:
+    log_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "LOGS_FOR_AGENTS.md")
     timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
-    line = f"{timestamp} | {message}"
-    with open(log_path, "a", encoding="utf-8") as log_file:
-        log_file.write(line + "\n")
+    full_line = f"{timestamp} | {line}"
+    try:
+        with open(log_path, "a", encoding="utf-8") as log_file:
+            log_file.write(full_line + "\n")
+    except Exception as exc:
+        print(f"[LOG][WARN] Не удалось записать LOGS_FOR_AGENTS.md: {exc}")
 
 TAU = data["tau"]
 LR = data["lr"]
 GAMMA = data["gamma"]
 NET_TYPE = "dueling" if DUELING_ENABLED else "basic"
+CLIP_REWARD = os.getenv("CLIP_REWARD", "off")
+GRAD_CLIP_VALUE = 100.0
 
 # ============================================================
 # (C) Несколько обучающих апдейтов на один шаг среды
 # ============================================================
 UPDATES_PER_STEP = int(data.get("updates_per_step", 1))  # 1 = как было раньше
 WARMUP_STEPS     = int(data.get("warmup_steps", 0))      # 0 = без прогрева
 
-append_log_for_agents(
-    "[TRAIN] "
-    f"DoubleDQN={int(DOUBLE_DQN_ENABLED)} "
-    f"Dueling={int(DUELING_ENABLED)} "
-    f"LR={LR} GAMMA={data.get('gamma')}"
-)
-append_log_for_agents(
-    "[TRAIN] "
-    f"PER={int(PER_ENABLED)} "
-    f"alpha={PER_ALPHA} "
-    f"beta_start={PER_BETA_START} "
-    f"N_STEP={N_STEP}"
-)
+if TRAIN_LOG_ENABLED:
+    train_start_line = (
+        "[TRAIN][START] "
+        f"DoubleDQN={int(DOUBLE_DQN_ENABLED)} "
+        f"Dueling={int(DUELING_ENABLED)} "
+        f"PER={int(PER_ENABLED)} "
+        f"N_STEP={N_STEP} "
+        f"LR={LR} "
+        f"clip_reward={CLIP_REWARD} "
+        f"grad_clip={GRAD_CLIP_VALUE}"
+    )
+    if TRAIN_LOG_TO_FILE:
+        append_agent_log(train_start_line)
+    if TRAIN_LOG_TO_CONSOLE:
+        print(train_start_line)
 
 b_len = 60
 b_hei = 40
 
 print("\nTraining...\n")
 
 enemy1 = Unit(unitData("Space_Marine", "Eliminator Squad"), weaponData("Bolt Pistol"), weaponData("Close combat weapon"), b_len, b_hei)
 model1 = Unit(unitData("Space_Marine", "Eliminator Squad"), weaponData("Bolt Pistol"), weaponData("Close combat weapon"), b_len, b_hei)
 
 enemy2 = Unit(unitData("Space_Marine", "Apothecary"), weaponData("Absolver Bolt Pistol"), weaponData("Close combat weapon"), b_len, b_hei)
 model2 = Unit(unitData("Space_Marine", "Apothecary"), weaponData("Absolver Bolt Pistol"), weaponData("Close combat weapon"), b_len, b_hei)
 
 enemy = [enemy1, enemy2]
 model = [model1, model2]
 
 end = False
 trunc = True
 totLifeT = 10
 steps_done = 0
 
 if os.path.isfile("gui/data.json"):
 
     totLifeT = initFile.getNumLife()
     b_len = initFile.getBoardX()
     b_hei = initFile.getBoardY()
@@ -378,64 +398,64 @@ policy_net = DQN(n_observations, n_actions, dueling=DUELING_ENABLED).to(device)
 target_net = DQN(n_observations, n_actions, dueling=DUELING_ENABLED).to(device)
 target_net.load_state_dict(policy_net.state_dict())
 target_net.eval()
 
 opponent_policy_net = None
 if SELF_PLAY_ENABLED:
     opponent_policy_net = DQN(n_observations, n_actions, dueling=DUELING_ENABLED).to(device)
     opponent_policy_net.eval()
     if SELF_PLAY_OPPONENT_MODE == "fixed_checkpoint":
         if not SELF_PLAY_FIXED_PATH:
             raise ValueError("SELF_PLAY_FIXED_PATH обязателен для режима fixed_checkpoint.")
         if not os.path.isfile(SELF_PLAY_FIXED_PATH):
             raise FileNotFoundError(
                 f"SELF_PLAY_FIXED_PATH не найден: {SELF_PLAY_FIXED_PATH}. Проверь путь."
             )
         checkpoint = torch.load(SELF_PLAY_FIXED_PATH, map_location=device)
         if isinstance(checkpoint, dict) and "policy_net" in checkpoint:
             checkpoint_net_type = checkpoint.get("net_type", "basic")
             if checkpoint_net_type != NET_TYPE:
                 warn_msg = (
                     "[SELFPLAY] ВНИМАНИЕ: несовпадение типа сети "
                     f"(checkpoint={checkpoint_net_type}, текущая={NET_TYPE}). "
                     "Стартуем с новой инициализацией."
                 )
                 print(warn_msg)
-                append_log_for_agents(warn_msg)
+                append_agent_log(warn_msg)
             else:
                 opponent_policy_net.load_state_dict(checkpoint["policy_net"])
         else:
             if NET_TYPE != "basic":
                 warn_msg = (
                     "[SELFPLAY] ВНИМАНИЕ: старый формат чекпойнта без net_type, "
                     f"а текущая сеть={NET_TYPE}. Стартуем с новой инициализацией."
                 )
                 print(warn_msg)
-                append_log_for_agents(warn_msg)
+                append_agent_log(warn_msg)
             else:
                 opponent_policy_net.load_state_dict(checkpoint)
-        append_log_for_agents(
+        append_agent_log(
             f"[SELFPLAY] fixed_checkpoint path={SELF_PLAY_FIXED_PATH}"
         )
     else:
         opponent_policy_net.load_state_dict(policy_net.state_dict())
 
 optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)
 if PER_ENABLED:
     memory = PrioritizedReplayMemory(10000, alpha=PER_ALPHA, eps=PER_EPS)
 else:
     memory = ReplayMemory(10000)
 
 def opponent_policy(obs):
     if opponent_policy_net is None:
         return None
     action = select_action_with_epsilon(
         env,
         obs,
         opponent_policy_net,
         SELF_PLAY_OPPONENT_EPSILON,
         len(model),
     )
     return convertToDict(action)
 
 inText = []
 
@@ -447,71 +467,71 @@ for i in enemy:
     inText.append("Name: {}, Army Type: {}".format(i.showUnitData()["Name"], i.showUnitData()["Army"]))
 inText.append("Number of Lifetimes ran: {}\n".format(totLifeT))
 
 i = 0
 
 pbar = tqdm(total=totLifeT)
 
 state, info = env.reset(m=model, e=enemy, Type="big", trunc=True)
 
 current_time = datetime.datetime.now()
 date = str(current_time.second)+"-"+str(current_time.microsecond)
 name = "M:"+model[0].showUnitData()["Army"]+"_vs_"+"P:"+enemy[0].showUnitData()["Army"]
 fold =  "models/"+name
 fileName = fold+"/model-"+date+".pickle"
 randNum = np.random.randint(0, 10000000)
 metrics = metrics(fold, randNum, date)
 
 rewArr = []
 ep_rows = [] 
 
 epLen = 0
 n_step_buffer = collections.deque(maxlen=N_STEP)
 optimize_steps = 0
 initial_model_hp = float(sum(getattr(env, "unit_health", [])))
 initial_enemy_hp = float(sum(getattr(env, "enemy_health", [])))
-append_log_for_agents(
+append_agent_log(
     "Старт обучения: "
     f"model_hp_total={initial_model_hp}, enemy_hp_total={initial_enemy_hp}, "
     f"battle_round={getattr(env, 'battle_round', 'n/a')}, trunc={trunc}"
 )
 if trunc:
-    append_log_for_agents(
+    append_agent_log(
         "Логи фаз/ходов отключены (trunc=True). "
         "Чтобы включить подробные логи: VERBOSE_LOGS=1 или MANUAL_DICE=1."
     )
 if initial_model_hp <= 0 or initial_enemy_hp <= 0:
-    append_log_for_agents(
+    append_agent_log(
         "ВНИМАНИЕ: на старте эпизода обнаружено нулевое здоровье. "
         f"model_hp_total={initial_model_hp}, enemy_hp_total={initial_enemy_hp}. "
         "Это может приводить к мгновенному завершению эпизодов."
     )
 
 while end == False:
     epLen += 1
     if SELF_PLAY_ENABLED and epLen == 1:
-        append_log_for_agents(
+        append_agent_log(
             f"Старт эпизода {numLifeT + 1}. "
             f"[SELFPLAY] enabled=1 mode={SELF_PLAY_OPPONENT_MODE} "
             f"update_every={SELF_PLAY_UPDATE_EVERY_EPISODES} opp_eps={SELF_PLAY_OPPONENT_EPSILON}"
         )
     action = select_action(env, state, i, policy_net, len(model))
     action_dict = convertToDict(action)
     if trunc == False:
         print(env.get_info())
 
     if SELF_PLAY_ENABLED:
         env.enemyTurn(trunc=trunc, policy_fn=opponent_policy)
     else:
         env.enemyTurn(trunc=trunc)
     next_observation, reward, done, res, info = env.step(action_dict)
     rewArr.append(float(reward))
 
 
     unit_health = info["model health"]
     enemy_health = info["player health"]
     inAttack = info["in attack"]
 
     if inAttack == 1:
         if trunc == False:
             print("The units are fighting")
 
@@ -539,133 +559,172 @@ while end == False:
 
     n_step_buffer.append((state_t, action, float(reward), next_state_t, done))
     if len(n_step_buffer) >= N_STEP:
         reward_sum, n_step_next_state, n_step_count = build_n_step_transition(
             n_step_buffer, GAMMA
         )
         reward_sum_t = torch.tensor([reward_sum], device=dev, dtype=torch.float32)
         head_state, head_action, _, _, _ = n_step_buffer[0]
         memory.push(head_state, head_action, n_step_next_state, reward_sum_t, n_step_count)
         n_step_buffer.popleft()
     if done:
         while n_step_buffer:
             reward_sum, n_step_next_state, n_step_count = build_n_step_transition(
                 n_step_buffer, GAMMA
             )
             reward_sum_t = torch.tensor([reward_sum], device=dev, dtype=torch.float32)
             head_state, head_action, _, _, _ = n_step_buffer[0]
             memory.push(head_state, head_action, n_step_next_state, reward_sum_t, n_step_count)
             n_step_buffer.popleft()
     state = next_observation
 
     # =========================
     # ✅ Несколько обучающих апдейтов на 1 шаг среды
     losses = []
     last_td_stats = None
+    last_per_beta = PER_BETA_START
+    last_loss_value = None
 
     if i >= WARMUP_STEPS:
         for _ in range(UPDATES_PER_STEP):
             per_beta = PER_BETA_START
             if PER_ENABLED and PER_BETA_FRAMES > 0:
                 per_beta = min(
                     1.0,
                     PER_BETA_START
                     + (1.0 - PER_BETA_START)
                     * (optimize_steps / float(PER_BETA_FRAMES)),
                 )
             result = optimize_model(
                 policy_net,
                 target_net,
                 optimizer,
                 memory,
                 n_observations,
                 double_dqn_enabled=DOUBLE_DQN_ENABLED,
                 per_enabled=PER_ENABLED,
                 per_beta=per_beta,
                 per_eps=PER_EPS,
             )
 
             # optimize_model возвращает 0 если replay ещё маленький — такие пропускаем
             if result and result["loss"] != 0:
                 losses.append(result["loss"])
                 last_td_stats = result
+                last_per_beta = per_beta
+                last_loss_value = result["loss"]
                 optimize_steps += 1
+                if TRAIN_LOG_ENABLED and optimize_steps % TRAIN_LOG_EVERY_UPDATES == 0:
+                    eps_value = EPS_END + (EPS_START - EPS_END) * math.exp(
+                        -1.0 * i / EPS_DECAY
+                    )
+                    train_line = (
+                        "[TRAIN] "
+                        f"ep={numLifeT + 1} "
+                        f"upd={optimize_steps} "
+                        f"step={i} "
+                        f"loss={last_loss_value:.6f} "
+                        f"eps={eps_value:.4f} "
+                        f"lr={LR:.6g} "
+                        f"gamma={GAMMA:.6g} "
+                        f"PER={int(PER_ENABLED)} "
+                        f"alpha={PER_ALPHA:.4g} "
+                        f"beta={last_per_beta:.4g} "
+                        f"N_STEP={N_STEP}"
+                    )
+                    if N_STEP > 1:
+                        effective_gamma = GAMMA ** N_STEP
+                        train_line += f" effective_gamma={effective_gamma:.6g}"
+                    if PER_ENABLED and last_td_stats.get("per_stats"):
+                        per_stats = last_td_stats["per_stats"]
+                        train_line += (
+                            f" td_abs_mean={per_stats['td_error_mean']:.6f}"
+                            f" td_abs_max={per_stats['td_error_max']:.6f}"
+                            f" prio_mean={per_stats['priority_mean']:.6f}"
+                            f" prio_max={per_stats['priority_max']:.6f}"
+                            f" isw_mean={per_stats['is_weight_mean']:.6f}"
+                            f" isw_max={per_stats['is_weight_max']:.6f}"
+                        )
+                    if TRAIN_LOG_TO_FILE:
+                        append_agent_log(train_line)
+                    if TRAIN_LOG_TO_CONSOLE:
+                        print(train_line)
 
             # ✅ Быстрый soft-update target_net (намного быстрее, чем state_dict)
             with torch.no_grad():
                 for p_tgt, p in zip(target_net.parameters(), policy_net.parameters()):
                     p_tgt.data.mul_(1.0 - TAU)
                     p_tgt.data.add_(p.data, alpha=TAU)
 
     # чтобы график loss не раздувался в 100 раз — пишем среднее за env-step
     if len(losses) > 0:
         metrics.updateLoss(sum(losses) / len(losses))
     else:
         metrics.updateLoss(0)
     if REWARD_DEBUG and last_td_stats and optimize_steps % REWARD_DEBUG_EVERY == 0:
-        append_log_for_agents(
+        append_agent_log(
             "[TD] "
             f"step={i} "
             f"mean={last_td_stats['td_target_mean']:.6f} "
             f"max={last_td_stats['td_target_max']:.6f}"
         )
         if PER_ENABLED and last_td_stats.get("per_stats"):
             per_stats = last_td_stats["per_stats"]
-            append_log_for_agents(
+            append_agent_log(
                 "[PER] "
                 f"opt_step={optimize_steps} "
                 f"priority_mean={per_stats['priority_mean']:.6f} "
                 f"priority_max={per_stats['priority_max']:.6f} "
                 f"is_weight_mean={per_stats['is_weight_mean']:.6f} "
                 f"td_error_mean={per_stats['td_error_mean']:.6f} "
                 f"td_error_max={per_stats['td_error_max']:.6f}"
             )
     # =========================
 
 
 
     if done == True:
         if SELF_PLAY_ENABLED:
-            append_log_for_agents(
+            append_agent_log(
                 f"Конец эпизода {numLifeT + 1}. "
                 f"[SELFPLAY] enabled=1 mode={SELF_PLAY_OPPONENT_MODE} "
                 f"update_every={SELF_PLAY_UPDATE_EVERY_EPISODES} opp_eps={SELF_PLAY_OPPONENT_EPSILON}"
             )
         end_reason_env = info.get("end reason", "")
         winner_env = info.get("winner")
         model_hp_total = sum(info.get("model health", [])) if isinstance(info.get("model health"), (list, tuple, np.ndarray)) else info.get("model health")
         enemy_hp_total = sum(info.get("player health", [])) if isinstance(info.get("player health"), (list, tuple, np.ndarray)) else info.get("player health")
-        append_log_for_agents(
+        append_agent_log(
             "Конец эпизода: "
             f"reason={end_reason_env or 'unknown'} "
             f"winner={winner_env} "
             f"model_hp_total={model_hp_total} enemy_hp_total={enemy_hp_total} "
             f"model_vp={info.get('model VP')} enemy_vp={info.get('player VP')} "
             f"turn={info.get('turn')} battle_round={info.get('battle round')}"
         )
         if epLen == 1:
-            append_log_for_agents(
+            append_agent_log(
                 "ВНИМАНИЕ: эпизод завершился на первом шаге. "
                 "Проверьте reset/условия завершения (нулевое здоровье, лимиты хода, "
                 "ошибки расстановки)."
             )
         pbar.update(1)
         metrics.updateRew(sum(rewArr)/len(rewArr))
         metrics.updateEpLen(epLen)
         # ===== extra metrics (winrate / VP diff / end reason) =====
         ep_reward = float(sum(rewArr) / len(rewArr)) if len(rewArr) > 0 else 0.0
         model_vp = int(info.get("model VP", 0))
         player_vp = int(info.get("player VP", 0))
         vp_diff = model_vp - player_vp
 
         mh_list = info.get("model health", [])
         ph_list = info.get("player health", [])
 
         def _sum_health(x):
             try:
                 if isinstance(x, (list, tuple, np.ndarray)):
                     return int(sum(x))
                 return int(x)
             except Exception:
                 return 0
 
         mh = _sum_health(mh_list)
@@ -702,87 +761,101 @@ while end == False:
             result = "loss"
             end_reason = "wipe_model"
         else:
             mission_name = info.get("mission", DEFAULT_MISSION_NAME)
             end_reason = f"turn_limit_{mission_name}"
             if vp_diff > 0:
                 result = "win"
             elif vp_diff < 0:
                 result = "loss"
             else:
                 result = "draw"
 
         if result == "win":
             inText.append("model won!")
             if trunc == False:
                 print("model won!")
         elif result == "loss":
             inText.append("enemy won!")
             if trunc == False:
                 print("enemy won!")
         else:
             inText.append("draw!")
             if trunc == False:
                 print("draw!")
 
+        if TRAIN_LOG_ENABLED:
+            win_flag = 1 if result == "win" else 0
+            train_ep_line = (
+                "[TRAIN][EP] "
+                f"ep={numLifeT + 1} "
+                f"ep_reward={ep_reward:.6f} "
+                f"win={win_flag} "
+                f"vp_diff={vp_diff} "
+                f"end_reason={end_reason}"
+            )
+            if TRAIN_LOG_TO_FILE:
+                append_agent_log(train_ep_line)
+            if TRAIN_LOG_TO_CONSOLE:
+                print(train_ep_line)
 
         ep_rows.append({
             "episode": numLifeT + 1,   # lifetimes считаются у тебя через numLifeT
             "ep_reward": ep_reward,
             "ep_len": epLen,
             "turn": turn,
             "model_vp": model_vp,
             "player_vp": player_vp,
             "vp_diff": vp_diff,
             "result": result,
             "end_reason": end_reason,
             "end_code": end_code,
         })
         # ==========================================================
 
         epLen = 0
         rewArr = []
 
         if res == 4:
             inText.append("Major Victory")
 
         if float(reward) > 0:
             inText.append("model won!")
             if trunc == False:
                 print("model won!")
         else:
             inText.append("enemy won!")
             if trunc == False:
                 print("enemy won!")
         if trunc == False:
             print("Restarting...")
         numLifeT+=1
 
         if SELF_PLAY_ENABLED and SELF_PLAY_OPPONENT_MODE == "snapshot":
             if numLifeT % SELF_PLAY_UPDATE_EVERY_EPISODES == 0:
                 opponent_policy_net.load_state_dict(policy_net.state_dict())
-                append_log_for_agents(
+                append_agent_log(
                     f"[SELFPLAY] opponent snapshot updated at episode {numLifeT}"
                 )
 
         attacker_side, defender_side = roll_off_attacker_defender(
             manual_roll_allowed=False,
             log_fn=print,
         )
         if verbose:
             print(f"[MISSION Only War] Attacker={attacker_side}, Defender={defender_side}")
 
         deploy_only_war(
             model_units=model,
             enemy_units=enemy,
             b_len=b_len,
             b_hei=b_hei,
             attacker_side=attacker_side,
             log_fn=log_fn,
         )
         post_deploy_setup(log_fn=log_fn)
         env.attacker_side = attacker_side
         env.defender_side = defender_side
 
         state, info = env.reset(m=model, e=enemy, Type="small", trunc=True)
 
     if numLifeT == totLifeT:

