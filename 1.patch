diff --git a/gym_mod/gym_mod/engine/mission.py b/gym_mod/gym_mod/engine/mission.py
index 536ec1f22938134b1c7113a93e7af32b70713563..8d6fbe5b52adeb7ea58b7fe2e23c6b815e23f2a7 100644
--- a/gym_mod/gym_mod/engine/mission.py
+++ b/gym_mod/gym_mod/engine/mission.py
@@ -1,36 +1,38 @@
 """Only War mission logic."""
 from __future__ import annotations
 
 from typing import Callable, List, Tuple
 
+import reward_config as reward_cfg
+
 MISSION_NAME = "Only War"
 # TODO(Only War): support post-deploy units ("set up after both armies deployed").
 # Currently no post-deploy units supported.
 MAX_BATTLE_ROUNDS = 10
-START_SCORING_ROUND = 2
-VP_CAP_PER_COMMAND = 3
+START_SCORING_ROUND = reward_cfg.VP_START_SCORING_ROUND
+VP_CAP_PER_COMMAND = reward_cfg.VP_CAP_PER_COMMAND
 
 
 def controlled_objectives(env, side: str) -> Tuple[int, List[int]]:
     """
     Returns (count, indices) of objectives controlled by the given side.
     Uses current OC totals on each objective.
     """
     if side not in ("model", "enemy"):
         raise ValueError(f"Unknown side: {side}")
 
     model_totals = getattr(env, "model_obj_oc", None)
     enemy_totals = getattr(env, "enemy_obj_oc", None)
     if model_totals is None or enemy_totals is None:
         raise AttributeError("Objective OC totals not available on env")
 
     controlled = []
     for idx in range(len(model_totals)):
         model_oc = int(model_totals[idx])
         enemy_oc = int(enemy_totals[idx])
         if side == "model" and model_oc > enemy_oc:
             controlled.append(idx)
         elif side == "enemy" and enemy_oc > model_oc:
             controlled.append(idx)
 
     return len(controlled), controlled
diff --git a/gym_mod/gym_mod/envs/warhamEnv.py b/gym_mod/gym_mod/envs/warhamEnv.py
index 38b1f95b332ae2cdb0fc11553d65ee51c36b3c8a..7dc264e99c38c24f89e85faaac93de8c839a57eb 100644
--- a/gym_mod/gym_mod/envs/warhamEnv.py
+++ b/gym_mod/gym_mod/envs/warhamEnv.py
@@ -1526,175 +1526,186 @@ class Warhammer40kEnv(gym.Env):
                         self.enemyOC[i] = self.enemy_data[i]["OC"]
                     else:
                         battleSh = True
                         self.enemyOC[i] = 0
                         if self.trunc is False:
                             self._log(f"{unit_label}: тест Battle-shock провален.")
                         if use_cp == 1 and cp_on == i and self.enemyCP - 1 >= 0:
                             battleSh = False
                             self.enemyCP -= 1
                             self.enemyOC[i] = self.enemy_data[i]["OC"]
 
                 battle_shock[i] = battleSh
             dice_fn = player_dice if os.getenv("MANUAL_DICE", "0") == "1" and side == "enemy" else auto_dice
             apply_end_of_command_phase(self, side="enemy", dice_fn=dice_fn, log_fn=self._log)
             score_end_of_command_phase(self, "enemy", log_fn=self._log)
             return battle_shock
 
         return None
 
     def movement_phase(self, side: str, action=None, manual: bool = False, battle_shock=None):
         self.begin_phase(side, "movement")
         if side == "model":
             self._log_phase("MODEL", "movement")
             advanced_flags = [False] * len(self.unit_health)
             reward_delta = 0
+            objective_hold_delta = 0.0
+            objective_proximity_delta = 0.0
             for i in range(len(self.unit_health)):
                 modelName = i + 21
                 battleSh = battle_shock[i] if battle_shock else False
                 pos_before = tuple(self.unit_coords[i])
                 if self.unit_health[i] <= 0:
                     self._log_unit("MODEL", modelName, i, f"Юнит мертв, движение пропущено. Позиция: {pos_before}")
                     continue
                 if self.unitInAttack[i][0] == 0 and self.unit_health[i] > 0:
                     base_m = self.unit_data[i]["Movement"]
                     label = "move_num_" + str(i)
                     want = int(action[label])
                     advanced = (action["move"] != 4) and (want > base_m)
                     advance_roll = None
                     if advanced:
                         advance_roll = dice()
                         max_move = base_m + advance_roll
                     else:
                         max_move = base_m
                     movement = min(want, max_move)
 
                     if action["move"] == 0:
                         self.unit_coords[i][0] += movement
                     elif action["move"] == 1:
                         self.unit_coords[i][0] -= movement
                     elif action["move"] == 2:
                         self.unit_coords[i][1] -= movement
                     elif action["move"] == 3:
                         self.unit_coords[i][1] += movement
                     elif action["move"] == 4:
                         for j in range(len(self.coordsOfOM)):
                             if distance(self.unit_coords[i], self.coordsOfOM[j]) <= 5:
-                                reward_delta += 0.5
+                                reward_delta += reward_cfg.VP_OBJECTIVE_HOLD_REWARD
+                                objective_hold_delta += reward_cfg.VP_OBJECTIVE_HOLD_REWARD
                             else:
-                                reward_delta -= 0.5
+                                reward_delta -= reward_cfg.VP_OBJECTIVE_HOLD_PENALTY
+                                objective_hold_delta -= reward_cfg.VP_OBJECTIVE_HOLD_PENALTY
 
                     advanced_flags[i] = advanced
                     direction = {0: "down", 1: "up", 2: "left", 3: "right", 4: "none"}.get(action["move"], "none")
                     actual_movement = movement if action["move"] != 4 else 0
                     advance_text = "да" if advanced else "нет"
                     if advance_roll is not None:
                         advance_detail = f", бросок={advance_roll}, макс={max_move}"
                     else:
                         advance_detail = ""
                     self._log_unit(
                         "MODEL",
                         modelName,
                         i,
                         f"Позиция до: {pos_before}. Выбор: {direction}, advance={advance_text}{advance_detail}, distance={actual_movement}",
                     )
 
                     self.unit_coords[i] = bounds(self.unit_coords[i], self.b_len, self.b_hei)
                     for j in range(len(self.enemy_health)):
                         if self.unit_coords[i] == self.enemy_coords[j]:
                             self.unit_coords[i][0] -= 1
                     pos_after = tuple(self.unit_coords[i])
                     if action["move"] == 4:
                         self._log_unit("MODEL", modelName, i, f"Движение пропущено (no move). Позиция после: {pos_after}")
                     else:
                         self._log_unit("MODEL", modelName, i, f"Позиция после: {pos_after}")
 
                     if pos_before != pos_after:
                         self._resolve_overwatch(
                             defender_side="enemy",
                             moving_unit_side="model",
                             moving_idx=i,
                             phase="movement",
                             manual=os.getenv("MANUAL_DICE", "0") == "1",
                         )
 
                     for j in range(len(self.coordsOfOM)):
                         if distance(self.coordsOfOM[j], self.unit_coords[i]) <= 5:
-                            reward_delta += 0.5
+                            reward_delta += reward_cfg.VP_OBJECTIVE_PROXIMITY_REWARD
+                            objective_proximity_delta += reward_cfg.VP_OBJECTIVE_PROXIMITY_REWARD
 
                 elif self.unitInAttack[i][0] == 1 and self.unit_health[i] > 0:
                     idOfE = self.unitInAttack[i][1]
                     if self.enemy_health[idOfE] <= 0:
                         reward_delta += 0.3
                         self.unitInAttack[i][0] = 0
                         self.unitInAttack[i][1] = 0
                         self.enemyInAttack[idOfE][0] = 0
                         self.enemyInAttack[idOfE][1] = 0
                         self._log_unit(
                             "MODEL",
                             modelName,
                             i,
                             f"Цель в ближнем бою мертва ({self._format_unit_label('enemy', idOfE)}), юнит выходит из боя. Позиция: {pos_before}",
                         )
                     else:
                         if action["attack"] == 0:
                             if self.unit_health[i] * 2 >= self.enemy_health[idOfE]:
                                 reward_delta -= 0.5
                             self._log_unit(
                                 "MODEL",
                                 modelName,
                                 i,
                                 f"Отступление из боя с {self._format_unit_label('enemy', idOfE)}. Позиция до: {pos_before}",
                             )
                             self.unitFellBack[i] = True
                             if battleSh is True:
                                 diceRoll = dice()
                                 if diceRoll < 3:
                                     self.unit_health[i] -= self.unit_data[i]["W"]
                             self.unit_coords[i][0] += self.unit_data[i]["Movement"]
                             self.unitInAttack[i][0] = 0
                             self.unitInAttack[i][1] = 0
                             self.enemyInAttack[idOfE][0] = 0
                             self.enemyInAttack[idOfE][1] = 0
                             pos_after = tuple(self.unit_coords[i])
                             self._log_unit("MODEL", modelName, i, f"Отступление завершено. Позиция после: {pos_after}")
                             if pos_before != pos_after:
                                 self._resolve_overwatch(
                                     defender_side="enemy",
                                     moving_unit_side="model",
                                     moving_idx=i,
                                     phase="movement",
                                     manual=os.getenv("MANUAL_DICE", "0") == "1",
                                 )
                         else:
                             reward_delta += 0.2
-                            self._log_unit(
-                                "MODEL",
-                                modelName,
-                                i,
-                                f"Остаётся в ближнем бою с {self._format_unit_label('enemy', idOfE)}, движение пропущено.",
-                            )
+                        self._log_unit(
+                            "MODEL",
+                            modelName,
+                            i,
+                            f"Остаётся в ближнем бою с {self._format_unit_label('enemy', idOfE)}, движение пропущено.",
+                        )
+            if objective_hold_delta != 0 or objective_proximity_delta != 0:
+                total_obj_delta = objective_hold_delta + objective_proximity_delta
+                self._log(
+                    "Reward (VP/объекты, движение): "
+                    f"hold={objective_hold_delta:.3f}, proximity={objective_proximity_delta:.3f}, total={total_obj_delta:.3f}"
+                )
             return advanced_flags, reward_delta
 
         if side == "enemy" and action is not None and not manual:
             self._log_phase(self._display_side("enemy"), "movement")
             advanced_flags = [False] * len(self.enemy_health)
             move_dir = action.get("move", 4) if isinstance(action, dict) else 4
             attack_choice = action.get("attack", 1) if isinstance(action, dict) else 1
             for i in range(len(self.enemy_health)):
                 unit_id = i + 11
                 battleSh = battle_shock[i] if battle_shock else False
                 pos_before = tuple(self.enemy_coords[i])
                 if self.enemy_health[i] <= 0:
                     self._log_unit("enemy", unit_id, i, f"Юнит мертв, движение пропущено. Позиция: {pos_before}")
                     continue
                 if self.enemyInAttack[i][0] == 0 and self.enemy_health[i] > 0:
                     base_m = self.enemy_data[i]["Movement"]
                     label = "move_num_" + str(i)
                     want = int(action.get(label, base_m)) if isinstance(action, dict) else base_m
                     advanced = (move_dir != 4) and (want > base_m)
                     advance_roll = None
                     if advanced:
                         advance_roll = dice()
                         max_move = base_m + advance_roll
                     else:
                         max_move = base_m
@@ -2818,50 +2829,55 @@ class Warhammer40kEnv(gym.Env):
                             strength_term += reward_cfg.MELEE_STRENGTH_SCALE
                         elif model_power < enemy_power:
                             strength_term -= reward_cfg.MELEE_STRENGTH_SCALE
         self.resolve_fight_phase(active_side=side, trunc=self.trunc)
 
         if side == "model" and engaged_pairs:
             post_enemy_hp_total = float(sum(self.enemy_health))
             post_model_hp_total = float(sum(self.unit_health))
             post_enemy_dead = sum(1 for hp in self.enemy_health if hp <= 0)
 
             damage_dealt = max(0.0, pre_enemy_hp_total - post_enemy_hp_total)
             damage_taken = max(0.0, pre_model_hp_total - post_model_hp_total)
             damage_dealt_norm = damage_dealt / max(1.0, float(self.enemy_hp_max_total))
             damage_taken_norm = damage_taken / max(1.0, float(self.model_hp_max_total))
             damage_term = reward_cfg.MELEE_REWARD_DAMAGE_SCALE * damage_dealt_norm
             taken_term = reward_cfg.MELEE_REWARD_TAKEN_SCALE * damage_taken_norm
             kill_delta = max(0, post_enemy_dead - pre_enemy_dead)
             kill_term = reward_cfg.MELEE_REWARD_KILL_BONUS * kill_delta
 
             self.refresh_objective_control()
             post_obj_controlled, _ = controlled_objectives(self, "model")
             obj_delta = post_obj_controlled - (pre_obj_controlled or 0)
             obj_term = reward_cfg.MELEE_OBJECTIVE_CONTROL_SCALE * obj_delta
 
             reward_delta += damage_term + kill_term - taken_term + advantage_term + strength_term + obj_term
+            if obj_term != 0:
+                self._log(
+                    "Reward (VP/объекты, бой): "
+                    f"delta={obj_delta}, term={obj_term:.3f}"
+                )
             self._log(
                 "Reward (бой): "
                 f"damage={damage_term:.3f} (norm={damage_dealt_norm:.3f}, dealt={damage_dealt:.2f}), "
                 f"kills={kill_term:.3f} (delta={kill_delta}), "
                 f"taken=-{taken_term:.3f} (norm={damage_taken_norm:.3f}, taken={damage_taken:.2f}), "
                 f"advantage={advantage_term:.3f}, strength={strength_term:.3f}, "
                 f"objectives={obj_term:.3f} (delta={obj_delta}), total={reward_delta:.3f}"
             )
         return reward_delta
 
     def refresh_objective_control(self):
         self.model_obj_oc = np.zeros(len(self.coordsOfOM), dtype=int)
         self.enemy_obj_oc = np.zeros(len(self.coordsOfOM), dtype=int)
 
         for i in range(len(self.unit_health)):
             if self.unit_health[i] <= 0:
                 continue
             wounds = self.unit_data[i]["W"]
             remaining_models = (self.unit_health[i] + wounds - 1) // wounds
             effective_oc = self.modelOC[i] * remaining_models
             if effective_oc <= 0:
                 continue
             for j in range(len(self.coordsOfOM)):
                 if distance(self.coordsOfOM[j], self.unit_coords[i]) <= 5:
                     self.model_obj_oc[j] += effective_oc
@@ -3299,53 +3315,55 @@ class Warhammer40kEnv(gym.Env):
         self.enemyStrat["smokescreen"] = -1
 
         for i in range(len(self.unit_health)):
             if self.unit_health[i] < 0:
                 self.unit_health[i] = 0
         for i in range(len(self.enemy_health)):
             if self.enemy_health[i] < 0:
                 self.enemy_health[i] = 0
 
         model_hp_end = float(sum(self.unit_health))
         damage_taken = max(0.0, model_hp_start - model_hp_end)
         if damage_taken > 0:
             damage_taken_norm = damage_taken / max(1.0, float(self.model_hp_max_total))
             penalty = reward_cfg.DAMAGE_TAKEN_SCALE * damage_taken_norm
             reward -= penalty
             self._log(
                 "Reward (урон по модели): "
                 f"damage_taken={damage_taken:.2f}, norm={damage_taken_norm:.3f}, penalty=-{penalty:.3f}"
             )
 
         if game_over:
             res = 4
             self.last_end_reason = end_reason
             self.last_winner = winner
             if winner == "model":
-                reward += 2
+                reward += reward_cfg.WIN_BONUS
+                self._log(f"Reward (победа): bonus=+{reward_cfg.WIN_BONUS:.3f}")
             elif winner == "enemy":
-                reward -= 2
+                reward -= reward_cfg.LOSS_PENALTY
+                self._log(f"Reward (поражение): penalty=-{reward_cfg.LOSS_PENALTY:.3f}")
 
         self._advance_turn_order()
         if self.game_over and res == 0:
             res = 4
 
         self.iter += 1
         if not self.game_over:
             self.last_end_reason = ""
             self.last_winner = None
         info = self.get_info()
         return self._get_observation(), reward, self.game_over, res, info
 
     def player(self):
         self.active_side = "enemy"
 
         info = self.get_info()
         self._log(str(info))
         more_info = "Здоровье MODEL: {}, здоровье PLAYER: {}\nCP MODEL: {}, CP PLAYER: {}\nVP MODEL: {}, VP PLAYER: {}\n".format(
             info["model health"],
             info["player health"],
             info["modelCP"],
             info["playerCP"],
             info["model VP"],
             info["player VP"],
         )
diff --git a/reward_config.py b/reward_config.py
index d688c9834bd45af055f69c70160602cfe60fda15..137e55decf6947c55547ff60b384a6e61136c822 100644
--- a/reward_config.py
+++ b/reward_config.py
@@ -1,30 +1,46 @@
 """
 Конфигурация reward-шэйпинга.
 Меняйте значения здесь, чтобы не лезть в код среды.
 """
 
+# ==========================
+# Reward shaping (win / VP)
+# ==========================
+# Бонус/штраф за результат боя.
+WIN_BONUS = 3.0
+LOSS_PENALTY = 2.0
+
+# Настройки начисления VP (миссия Only War).
+VP_START_SCORING_ROUND = 2
+VP_CAP_PER_COMMAND = 3
+
+# Награды, связанные с целями/объектами (VP-компоненты).
+VP_OBJECTIVE_HOLD_REWARD = 0.5
+VP_OBJECTIVE_HOLD_PENALTY = 0.5
+VP_OBJECTIVE_PROXIMITY_REWARD = 0.5
+
 # =========================
 # Reward shaping (shooting)
 # =========================
 # Базовые коэффициенты — подбирайте экспериментально.
 SHOOT_REWARD_DAMAGE_SCALE = 0.6
 SHOOT_REWARD_KILL_BONUS = 0.4
 SHOOT_REWARD_OVERKILL_PENALTY = 0.2
 SHOOT_REWARD_SKIP_PENALTY = 0.15
 SHOOT_REWARD_TARGET_LOW_HP = 0.05
 SHOOT_REWARD_TARGET_ON_OBJ = 0.07
 SHOOT_REWARD_TARGET_HIGH_OC = 0.05
 SHOOT_REWARD_ACTION_BONUS = 0.0
 
 # Penalize damage received during the model's step (normalized by model total max HP).
 DAMAGE_TAKEN_SCALE = 0.5
 
 # ======================
 # Reward shaping (fight)
 # ======================
 # Базовые коэффициенты — подбирайте экспериментально.
 MELEE_REWARD_DAMAGE_SCALE = 0.6
 MELEE_REWARD_KILL_BONUS = 0.4
 MELEE_REWARD_TAKEN_SCALE = 0.5
 MELEE_ADVANTAGE_SCALE = 0.15
 MELEE_STRENGTH_SCALE = 0.1

