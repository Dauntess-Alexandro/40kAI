diff --git a/train.py b/train.py
index c1e7fb1913e16d00202e6fcafebaf6d2e99b028a..1c242b4eb7f75fffcc6b883efe97b51e1a5cb174 100644
--- a/train.py
+++ b/train.py
@@ -664,59 +664,59 @@ while end == False:
                 print("draw!")
 
 
         ep_rows.append({
             "episode": numLifeT + 1,   # lifetimes считаются у тебя через numLifeT
             "ep_reward": ep_reward,
             "ep_len": epLen,
             "turn": turn,
             "model_vp": model_vp,
             "player_vp": player_vp,
             "vp_diff": vp_diff,
             "result": result,
             "end_reason": end_reason,
             "end_code": end_code,
         })
         # ==========================================================
 
         epLen = 0
         rewArr = []
 
         if res == 4:
             inText.append("Major Victory")
 
         if float(reward) > 0:
             inText.append("model won!")
-            if trunc == False:
+            if not trunc:
                 print("model won!")
         else:
             inText.append("enemy won!")
-            if trunc == False:
+            if not trunc:
                 print("enemy won!")
-        if trunc == False:
+        if not trunc:
             print("Restarting...")
-        numLifeT+=1
+        numLifeT += 1
         verbose_log.append(
             f"[episode] {numLifeT}/{totLifeT} reward={ep_reward:.3f} vp_diff={vp_diff} "
             f"eps={epsilon:.4f} loss={last_loss_mean:.6f} q={last_q_mean:.3f}"
         )
 
         if args.eval_interval > 0 and numLifeT % args.eval_interval == 0:
             winrate, avg_eval_rew, avg_vp = run_eval(policy_net, args.eval_episodes, args.eval_epsilon)
             print(f"[eval] winrate={winrate:.2f} avg_reward={avg_eval_rew:.2f} avg_vp_diff={avg_vp:.2f}")
             if winrate > best_eval_winrate:
                 best_eval_winrate = winrate
                 best_path = f"models/{name}/best-model-{date}.pth"
                 torch.save(
                     {
                         "policy_net": policy_net.state_dict(),
                         "target_net": target_net.state_dict(),
                         "optimizer": optimizer.state_dict(),
                         "metadata": {
                             "schema_version": 1,
                             "action_keys": ordered_keys,
                             "hyperparams": vars(args),
                         },
                     },
                     best_path,
                 )
                 print(f"[eval] new best -> {best_path}")

